# 强化学习--刘建平的博客

[刘建平博客发布地址](https://www.cnblogs.com/pinard/p/9385570.html)

----

## 强化学习（一） 模型基础

### 强化学习、监督学习、非监督学习的关系与区别

机器学习(Machine Learning)：强化学习(Reinforcement Learning)、监督学习(Supervised Learning)、非监督学习(Unsupervised Learning)

### 强化学习的建模

强化学习的模型要素：

1. 环境的状态$S$
2. 个体的动作$A$
3. 环境的奖励$R$
4. 个体的策略$\pi$。个体会根据策略$\pi$来选择动作。
5. 个体在策略$\pi$和状态$s$时，选择在动作后的价值$v_\pi(s)$
6. 奖励衰减因子$\gamma \in [0,1]$
7. 环境的状态转化模型。它可以表示为一个概率模型，及在状态$s$采取动作$a$，转移到下一个状态$s'$的概率，表示为$P^a_{ss'}$
8. 探索率$\epsilon $ 

### 强化学习的简单实例

[Github](https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/introduction.py)

## 强化学习（二） 马尔可夫决策过程（MDP）

### 马尔可夫性的假设

状态转化模型：$P^a_{ss′}=\mathbb{E}(S_{t+1}=s′|S_t=s,A_t=a)$

个体的策略：$\pi(a|s) = P(A_t = a | S_t = s)$

价值函数：$v_\pi(s) = \mathbb{E}_\pi(G_t|S_t = s) = \mathbb{E}_\pi(R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots | S_t = s)$

### MDP的价值函数与贝尔曼方程

状态价值函数：$v_\pi(s) = \mathbb{E}_\pi(G_t|S_t = s)$

动作价值函数 ：$q_\pi(s,a) = \mathbb{E}_\pi({G_t|S_t, A_t = a})$

状态价值函数的贝尔曼方程：$v_\pi(s) = \mathbb{E}_\pi(R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t =s)$

动作价值函数的贝尔曼方程：$q_\pi(s,a) = \mathbb{E}_\pi(R_{t+1} + \gamma q_\pi(S_{t+1},A_{t+1})|S_t = s, A_t = a)$

### 状态价值函数与动作价值函数的递推关系

$v_\pi(s) = \sum\limits_{a \in A}\pi(a|s)(R_s^a + \gamma \sum_{s' \in S}P_{ss'}^av_\pi(s'))$

$q_\pi(s,a) = R_s^a + \gamma \sum\limits_{s' \in S} P_{ss'}^a\sum\limits_{a' \in A}\pi(a'|s')q_\pi(s',a')$

### 最优价值函数

最优策略$\pi^\star$、最优价值函数$v_\star(s)$、最优动作价值函数$q_\star(s,a)$

$v_\star(s) = \max\limits_a(R_s^a + \gamma \sum\limits_{s' \in S}P_{ss'}^av_\star(s'))$

$q_\star(s,a) = R_s^a + \gamma \sum\limits_{s' \in S} P_{ss'}^a \max_a' q_\star(s',a')$

## 强化学习（三） 用动态规划（DP）求解

### 强化学习的两个基本问题

1. 预测问题，给定6个因素：状态集$S$、动作集$A$、即时奖励$R$、状态转移概率$P$、衰减因子$\gamma$、策略$\pi $，求该策略的状态价值函数$v(\pi)$
2. 控制问题，给定5个要素：状态集$S$、动作集$A$、即时奖励$R$、状态转移概率$P$、衰减因子$\gamma$，求最优策略$\pi_\star$和最优状态价值函数$v_\star$

### 动态规划的特点

基于模型

### 策略评估求解预测问题（Policy Evaluation）

### 策略迭代求解控制问题（Policy Iteration）

1. 第一步，使用当前策略$\pi_{*}$评估计算当前策略的最终状态价值$v_{*}$;
2. 第二步，根据状态价值$v_{*}$更新策略$\pi_{*}$

## 强化学习（四） 用蒙特卡洛法（MC）求解

### 不基于模型的强化学习问题

1. 预测问题，给定5个因素：状态集$S$、动作集$A$、即时奖励$R$、衰减因子$\gamma$、策略$\pi $，求该策略的状态价值函数$v(\pi)$
2. 控制问题，给定5个要素：状态集$S$、动作集$A$、即时奖励$R$、衰减因子$\gamma$，探索率$\epsilon$、求最优策略$\pi_\star$和最优状态价值函数$v_\star$

### 蒙特卡洛法的特点

不基于模型，需要经历完整的状态序列

### 蒙特卡罗法求解强化学习预测问题

### 蒙特卡罗法求解强化学习控制问题

## 强化学习（五） 用时序差分（TD）求解

### 时序差分法的特点

不基于模型，不依赖于完整状态序列















